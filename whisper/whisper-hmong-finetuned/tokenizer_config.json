{
  "add_prefix_space": false,
  "backend": "tokenizers",
  "bos_token": "<|endoftext|>",
  "clean_up_tokenization_spaces": true,
  "eos_token": "<|endoftext|>",
  "errors": "replace",
  "extra_special_tokens": [
    "<|endoftext|>",
    "<|startoftranscript|>",
    "<|en|>",
    "<|zh|>",
    "<|de|>",
    "<|es|>",
    "<|ru|>",
    "<|ko|>",
    "<|fr|>",
    "<|ja|>",
    "<|pt|>",
    "<|tr|>",
    "<|pl|>",
    "<|ca|>",
    "<|nl|>",
    "<|ar|>",
    "<|sv|>",
    "<|it|>",
    "<|id|>",
    "<|hi|>",
    "<|fi|>",
    "<|vi|>",
    "<|he|>",
    "<|uk|>",
    "<|el|>",
    "<|ms|>",
    "<|cs|>",
    "<|ro|>",
    "<|da|>",
    "<|hu|>",
    "<|ta|>",
    "<|no|>",
    "<|th|>",
    "<|ur|>",
    "<|hr|>",
    "<|bg|>",
    "<|lt|>",
    "<|la|>",
    "<|mi|>",
    "<|ml|>",
    "<|cy|>",
    "<|sk|>",
    "<|te|>",
    "<|fa|>",
    "<|lv|>",
    "<|bn|>",
    "<|sr|>",
    "<|az|>",
    "<|sl|>",
    "<|kn|>",
    "<|et|>",
    "<|mk|>",
    "<|br|>",
    "<|eu|>",
    "<|is|>",
    "<|hy|>",
    "<|ne|>",
    "<|mn|>",
    "<|bs|>",
    "<|kk|>",
    "<|sq|>",
    "<|sw|>",
    "<|gl|>",
    "<|mr|>",
    "<|pa|>",
    "<|si|>",
    "<|km|>",
    "<|sn|>",
    "<|yo|>",
    "<|so|>",
    "<|af|>",
    "<|oc|>",
    "<|ka|>",
    "<|be|>",
    "<|tg|>",
    "<|sd|>",
    "<|gu|>",
    "<|am|>",
    "<|yi|>",
    "<|lo|>",
    "<|uz|>",
    "<|fo|>",
    "<|ht|>",
    "<|ps|>",
    "<|tk|>",
    "<|nn|>",
    "<|mt|>",
    "<|sa|>",
    "<|lb|>",
    "<|my|>",
    "<|bo|>",
    "<|tl|>",
    "<|mg|>",
    "<|as|>",
    "<|tt|>",
    "<|haw|>",
    "<|ln|>",
    "<|ha|>",
    "<|ba|>",
    "<|jw|>",
    "<|su|>",
    "<|translate|>",
    "<|transcribe|>",
    "<|startoflm|>",
    "<|startofprev|>",
    "<|nocaptions|>",
    "<|notimestamps|>"
  ],
  "is_local": false,
  "language": "vi",
  "model_max_length": 1024,
  "pad_token": "<|endoftext|>",
  "predict_timestamps": false,
  "processor_class": "WhisperProcessor",
  "return_attention_mask": false,
  "task": "transcribe",
  "tokenizer_class": "WhisperTokenizer",
  "unk_token": "<|endoftext|>"
}
